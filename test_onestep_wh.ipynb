{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df653a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataset import LinearDynamicalDataset, WHDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly.graph_objects as go\n",
    "from transformer_onestep import GPTConfig, GPT\n",
    "import tqdm\n",
    "import argparse\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = Path(\"fig\")\n",
    "fig_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d446d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix all random sources to make script fully reproducible\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(430)\n",
    "system_seed = 430 # Controls the system generation\n",
    "data_seed = 0 # Control the input generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a235fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall settings\n",
    "out_dir = \"out\"\n",
    "\n",
    "# System settings\n",
    "nu = 1\n",
    "ny = 1\n",
    "#seq_len = 600\n",
    "batch_size = 32 # 256\n",
    "\n",
    "\n",
    "# Compute settings\n",
    "cuda_device = \"cuda:1\"\n",
    "no_cuda = True\n",
    "threads = 5\n",
    "compile = False\n",
    "\n",
    "# Configure compute\n",
    "torch.set_num_threads(threads) \n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device_name  = cuda_device if use_cuda else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "device_type = 'cuda' if 'cuda' in device_name else 'cpu' # for later use in torch.autocast\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "#torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create out dir\n",
    "out_dir = Path(out_dir)\n",
    "#exp_data = torch.load(out_dir/\"ckpt_onestep_wh_medium.pt\", map_location=device)\n",
    "exp_data = torch.load(out_dir/\"ckpt_onestep_wh_large.pt\", map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c029f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = exp_data[\"cfg\"].seq_len\n",
    "nx = exp_data[\"cfg\"].nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34332c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = exp_data[\"model_args\"]\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf).to(device)\n",
    "\n",
    "\n",
    "state_dict = exp_data[\"model\"]\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e08b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "#test_ds = LinearDynamicalDataset(nx=nx, nu=nu, ny=ny, seq_len=seq_len)\n",
    "test_ds = WHDataset(nx=nx, nu=nu, ny=ny, seq_len=seq_len, system_seed=system_seed, data_seed=data_seed, fixed_system=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y, batch_u = next(iter(test_dl))\n",
    "batch_y = batch_y.to(device)\n",
    "batch_u = batch_u.to(device)\n",
    "with torch.no_grad():\n",
    "    batch_y_pred, loss = model(batch_u, batch_y)\n",
    "    batch_y_pred = batch_y_pred.to(\"cpu\").detach().numpy()\n",
    "    batch_y = batch_y.to(\"cpu\").detach().numpy()\n",
    "    batch_u = batch_u.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6dd590",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_target = batch_y[:, 1:, :] # target @ time k: y_{k+1}\n",
    "batch_y_pred = batch_y_pred[:, :-1, :] # prediction @ time k: y_{k+1|k}\n",
    "batch_y_pred_dummy = batch_y[:, :-1, :] # dummy estimator: y_{k+1} \\approx y_{k}\n",
    "batch_pred_err = batch_y_target - batch_y_pred\n",
    "batch_pred_err_dummy = batch_y_target - batch_y_pred_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1, batch_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e22489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "\n",
    "idx = 11\n",
    "plt.plot(t, batch_y_target[idx], 'k', label=\"$y$\")\n",
    "plt.plot(t, batch_y_pred[idx], 'b', label=\"$\\hat y$\")\n",
    "#plt.plot(batch_y_pred_dummy[idx], 'm', label=\"Pred dummy\")\n",
    "plt.plot(t, batch_y_target[idx] - batch_y_pred[idx], 'r', label=\"$y - \\hat y$\")\n",
    "#plt.plot(batch_y_target[idx] - batch_y_pred_dummy[idx], 'm', label=\"Err dummy\")\n",
    "#plt.legend(loc=\"upper right\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.grid()\n",
    "plt.xlim([0, 100]);\n",
    "#plt.ylim([-3, 3]);\n",
    "plt.xlabel(\"time step (-)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path / \"wh_one_step_single.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aab2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "#plt.title(\"Prediction error $y - \\hat y$ on 256 sequences\")\n",
    "plt.plot(t, batch_y_target.squeeze(-1).T, \"k\", alpha=0.5);\n",
    "plt.plot(t, np.nan*np.zeros(t.shape), \"k\", alpha=1.0, label=\"$y$\")\n",
    "plt.plot(t, batch_pred_err.squeeze(-1).T, \"r\", alpha=0.2);\n",
    "plt.plot(t, np.nan*np.zeros(t.shape), \"r\", alpha=1.0, label=\"$y - \\hat y$\");\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"upper right\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel(\"time step (-)\")\n",
    "plt.tight_layout()\n",
    "#plt.xlabel(\"Time step $k$\");\n",
    "plt.savefig(fig_path / \"wh_one_step_batch.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34481aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 400\n",
    "rmse_transformer = metrics.rmse(batch_y_target[:, skip:, :], batch_y_pred[:, skip:, :], time_axis=1)\n",
    "rmse_dummy = metrics.rmse(batch_y_target[:, skip:, :], batch_y_pred_dummy[:, skip:, :], time_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'iterations: {exp_data[\"cfg\"].max_iters}')\n",
    "print(f'train_time: {exp_data[\"train_time\"] / 3600 / 24} days')\n",
    "print(f'rmse: {rmse_transformer.mean()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
