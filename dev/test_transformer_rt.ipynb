{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataset import LinearDynamicalDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import GPTConfig, GPT\n",
    "import tqdm\n",
    "import argparse\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c25428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall settings\n",
    "out_dir = \"out\"\n",
    "\n",
    "# System settings\n",
    "nx = 10\n",
    "nu = 1\n",
    "ny = 1\n",
    "seq_len = 400\n",
    "\n",
    "\n",
    "# Compute settings\n",
    "cuda_device = \"cuda:0\"\n",
    "no_cuda = False\n",
    "threads = 20\n",
    "compile = True\n",
    "batch_size = 256\n",
    "\n",
    "# Create out dir\n",
    "out_dir = Path(out_dir)\n",
    "exp_data = torch.load(out_dir/\"ckpt_lin.pt\")\n",
    "\n",
    "# Configure compute\n",
    "torch.set_num_threads(threads)\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device_name  = cuda_device if use_cuda else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "device_type = 'cuda' if 'cuda' in device_name else 'cpu' # for later use in torch.autocast\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "#torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "# Create data loader\n",
    "test_ds = LinearDynamicalDataset(nx=nx, nu=nu, ny=ny, seq_len=seq_len)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=threads)\n",
    "\n",
    "model_args = exp_data[\"model_args\"]\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "state_dict = exp_data[\"model\"]\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict);\n",
    "model = model.to(device)\n",
    "#if compile:\n",
    "#    model = torch.compile(model)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cad475",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y, batch_u = next(iter(test_dl))\n",
    "batch_y = batch_y.to(device)\n",
    "batch_u = batch_u.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56aae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model like in training (future inputs/outputs known)\n",
    "with torch.no_grad():\n",
    "    batch_y_pred, _ = model(batch_u, batch_y, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model causally to see if it really behaves correctly\n",
    "batch_y_pred_rt = []\n",
    "seq_len = batch_y.shape[1]\n",
    "with torch.no_grad():\n",
    "    for idx in range(seq_len):\n",
    "        batch_y_t, _ = model(batch_u[:, :idx+1, :], batch_y[:, :idx+1, :], compute_loss=False)\n",
    "        batch_y_pred_rt.append(batch_y_t)\n",
    "batch_y_pred_rt = torch.cat(batch_y_pred_rt, dim=1)\n",
    "#batch_y_pred_rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7277086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.testing.assert_close(batch_y_pred, batch_y_pred_rt)\n",
    "torch.max(torch.abs(batch_y_pred - batch_y_pred_rt)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model in simulation from a certain time step!\n",
    "sim_start = 100\n",
    "batch_y_sim = torch.zeros_like(batch_y)\n",
    "batch_y_sim[:, :sim_start, :] = batch_y[:, :sim_start, :]\n",
    "with torch.no_grad():\n",
    "    for idx in range(sim_start, seq_len):\n",
    "        batch_y_t, _ = model(batch_u[:, :idx, :], batch_y_sim[:, :idx, :], compute_loss=False)\n",
    "        batch_y_sim[:, [idx], :] = batch_y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_pred = batch_y_pred.to(\"cpu\").detach().numpy()\n",
    "batch_y_pred_rt = batch_y_pred_rt.to(\"cpu\").detach().numpy()\n",
    "batch_y_sim = batch_y_sim.detach().to(\"cpu\").numpy()\n",
    "batch_y = batch_y.detach().to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_target = batch_y[:, 1:, :] # target @ time k: y_{k+1}\n",
    "batch_y_pred = batch_y_pred[:, :-1, :] # prediction @ time k: y_{k+1|k}\n",
    "batch_y_sim = batch_y_sim[:, 1:, :] # simulation @ time k: y_{k+1|k}\n",
    "batch_pred_err = batch_y_target - batch_y_pred\n",
    "batch_sim_err = batch_y_target - batch_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9206d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 1\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=batch_y_target[instance].squeeze(), name=\"y\", line_color=\"black\"))\n",
    "fig.add_trace(go.Scatter(y=batch_y_sim[instance].squeeze(), name=\"y_sim\", line_color=\"blue\"))\n",
    "#fig.add_trace(go.Scatter(y=batch_y_pred[instance].squeeze(), name=\"y_pred\", line_color=\"magenta\"))\n",
    "fig.add_vline(x=sim_start, line_color=\"red\", name=\"sim_start\")\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot(batch_y[1], 'k', label=\"True\")\n",
    "#plt.plot(batch_y_pred[0], 'b', label=\"Pred\")\n",
    "#plt.plot(batch_y_pred_rt[0], 'm', label=\"Pred\")\n",
    "#plt.plot(batch_y_sim[1], 'b', label=\"Sim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchid import metrics\n",
    "skip = sim_start\n",
    "rmse_pred = metrics.rmse(batch_y_target[:, skip:, :], batch_y_pred[:, skip:, :], time_axis=1)\n",
    "rmse_sim = metrics.rmse(batch_y_target[:, skip:, :], batch_y_sim[:, skip:, :], time_axis=1)\n",
    "#rmse_z = metrics.rmse(batch_y_target[:, skip:, :], 0*batch_y_sim[:, skip:, :], time_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4bddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_pred.mean(), rmse_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"RMSE\")\n",
    "plt.hist(rmse_sim, color=\"black\", label=\"sim\");\n",
    "plt.hist(rmse_pred, color=\"red\", label=\"pred\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.boxplot([rmse_pred.ravel(), rmse_sim.ravel()], labels=[\"pred\", \"sim\"]);\n",
    "\n",
    "#plt.boxplot(rmse_pred);\n",
    "#ax.set_xticklabels(\"pred\")\n",
    "#plt.boxplot(rmse_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
